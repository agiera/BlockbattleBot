<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0//EN" "http://www.w3.org/TR/REC-html40/strict.dtd">

<html><head>
<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-133347-1";
urchinTracker();
</script>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"><title>Advanced Usage - Fast Artificial Neural Network Library (FANN)</title><link rel="stylesheet" type="text/css" href="../styles/main.css"><script language=JavaScript src="../javascript/main.js"></script></head><body class=UnframedPage onLoad="NDOnLoad()"><script language=JavaScript><!--
if (browserType) {document.write("<div class=" + browserType + ">");if (browserVer) {document.write("<div class=" + browserVer + ">"); }}// --></script>

<!--  Generated by Natural Docs, version 1.35 -->
<!--  http://www.naturaldocs.org  -->

<!-- saved from url=(0026)http://www.naturaldocs.org -->

<table border=0 cellspacing=0 cellpadding=0 width=100%><a name="top"><tr><td class=MenuSection valign=top><!--START_ND_MENU--><div class=MTitle>Fast Artificial Neural Network Library (FANN)<div class=MSubTitle>Reference Manual for latest CVS release</div></div><div class=MEntry><div class=MLink><a href="../../index.php">FANN HOME</a></div></div><div class=MEntry><div class=MGroup><a href="javascript:ToggleMenu('MGroupContent1')">Reference Manual</a><div class=MGroupContent id=MGroupContent1><div class=MEntry><div class=MFile><a href="../files/fann-h.html">FANN Creation/<span class=HB> </span>Execution</a></div></div><div class=MEntry><div class=MFile><a href="../files/fann_train-h.html">FANN Training</a></div></div><div class=MEntry><div class=MFile><a href="../files/fann_cascade-h.html">FANN Cascade Training</a></div></div><div class=MEntry><div class=MFile><a href="../files/fann_io-h.html">FANN File Input/<span class=HB> </span>Output</a></div></div><div class=MEntry><div class=MFile><a href="../files/fann_error-h.html">FANN Error Handling</a></div></div><div class=MEntry><div class=MFile><a href="../files/fann_data-h.html">FANN Datatypes</a></div></div><div class=MEntry><div class=MFile><a href="../files/fann_cpp-h.html">FANN Wrapper for C++</a></div></div></div></div></div><div class=MEntry><div class=MGroup><a href="javascript:ToggleMenu('MGroupContent2')">Tutorials</a><div class=MGroupContent id=MGroupContent2><div class=MEntry><div class=MFile><a href="installation-txt.html">Installing FANN</a></div></div><div class=MEntry><div class=MFile><a href="gettingstarted-txt.html">Getting Started</a></div></div><div class=MEntry><div class=MFile id=MSelected>Advanced Usage</div></div><div class=MEntry><div class=MFile><a href="fixedpointusage-txt.html">Fixed Point Usage</a></div></div><div class=MEntry><div class=MFile><a href="theory-txt.html">Neural Network Theory</a></div></div></div></div></div><div class=MEntry><div class=MLink><a href="../../index.php?p=download.php">Download FANN</a></div></div><div class=MEntry><div class=MGroup><a href="javascript:ToggleMenu('MGroupContent3')">Index</a><div class=MGroupContent id=MGroupContent3><div class=MEntry><div class=MIndex><a href="../index/General.html">Everything</a></div></div><div class=MEntry><div class=MIndex><a href="../index/Classes.html">Structs</a></div></div><div class=MEntry><div class=MIndex><a href="../index/Constants.html">Constants</a></div></div><div class=MEntry><div class=MIndex><a href="../index/Functions.html">Functions</a></div></div><div class=MEntry><div class=MIndex><a href="../index/Types.html">Types</a></div></div></div></div></div><!--END_ND_MENU--><center><script type="text/javascript"><!--
google_ad_client = "pub-7943942007953059";
google_ad_width = 120;
google_ad_height = 600;
google_ad_format = "120x600_as";
google_ad_type = "text_image";
google_ad_channel ="";
google_color_border = "E8E8E8";
google_color_bg = "E8E8E8";
google_color_link = "9D2933";
google_color_url = "9D2933";
google_color_text = "000000";
//--></script>
<script type="text/javascript"
  src="http://pagead2.googlesyndication.com/pagead/show_ads.js">
</script></center></td>

<td class=ContentSection valign=top><div class=CSection id=MainTopic><div class=CTopic><h1 class=CTitle><a name="Advanced_Usage"></a>Advanced Usage</h1><div class=CBody><p class=CParagraph>This section describes some of the low-level functions and how they can be used to obtain more control of the fann library.&nbsp; For a full list of functions, please see the Reference Manual, which has an explanation of all the fann library functions.&nbsp; Also feel free to take a look at the source code.</p><p class=CParagraph>This section describes different procedures, which can help to get more power out of the fann library.</p><!--START_ND_SUMMARY--><div class=Summary><div class=STitle>Summary</div><div class=SBorder><table border=0 cellspacing=0 cellpadding=0 class=STable><tr><td class=SEntrySize><div class=SMain><div class=SEntry><a href="#Advanced_Usage" >Advanced Usage</a></div></div></td><td class=SDescriptionSize><div class=SMain><div class=SDescription>This section describes some of the low-level functions and how they can be used to obtain more control of the fann library. </div></div></td></tr><tr class=SMarked><td><div class=SGeneric><div class=SEntry><div class=SIndent1><a href="#Adjusting_Parameters" >Adjusting Parameters</a></div></div></div></td><td><div class=SGeneric><div class=SDescription><div class=SIndent1>Several different parameters exists in an ANN, these parameters are given defaults in the fann library, but they can be adjusted at runtime. </div></div></div></td></tr><tr><td><div class=SGeneric><div class=SEntry><div class=SIndent1><a href="#Network_Design" >Network Design</a></div></div></div></td><td><div class=SGeneric><div class=SDescription><div class=SIndent1>When creating a network it is necessary to define how many layers, neurons and connections it should have. </div></div></div></td></tr><tr class=SMarked><td><div class=SGeneric><div class=SEntry><div class=SIndent1><a href="#Understanding_the_Error_Value" >Understanding the Error Value</a></div></div></div></td><td><div class=SGeneric><div class=SDescription><div class=SIndent1>The mean square error value is calculated while the ANN is being trained. </div></div></div></td></tr><tr><td><div class=SGeneric><div class=SEntry><div class=SIndent1><a href="#Training_and_Testing" >Training and Testing</a></div></div></div></td><td><div class=SGeneric><div class=SDescription><div class=SIndent1>Normally it will be sufficient to use the <a href="../files/fann_train-h.html#fann_train_on_file" class=LFunction id=link1 onMouseOver="ShowTip(event, 'tt1', 'link1')" onMouseOut="HideTip('tt1')">fann_train_on_file</a> training function, but sometimes you want to have more control and you will have to write a custom training loop. </div></div></div></td></tr><tr class=SMarked><td><div class=SGeneric><div class=SEntry><div class=SIndent1><a href="#Avoid_Over-Fitting" >Avoid Over-Fitting</a></div></div></div></td><td><div class=SGeneric><div class=SDescription><div class=SIndent1>With the knowledge of how to train and test an ANN, a new approach to training can be introduced. </div></div></div></td></tr><tr><td><div class=SGeneric><div class=SEntry><div class=SIndent1><a href="#Adjusting_Parameters_During_Training" >Adjusting Parameters During Training</a></div></div></div></td><td><div class=SGeneric><div class=SDescription><div class=SIndent1>If a very low mean square error is required it can sometimes be a good idea to gradually decrease the learning rate during training, in order to make the adjusting of weights more subtle. </div></div></div></td></tr></table><br><center><script type="text/javascript"><!--
google_ad_client = "pub-7943942007953059";
google_ad_width = 728;
google_ad_height = 90;
google_ad_format = "728x90_as";
google_ad_type = "text_image";
google_ad_channel ="";
google_color_border = "FFFFE0";
google_color_bg = "FFFFE0";
google_color_link = "9D2933";
google_color_url = "9D2933";
google_color_text = "000000";
//--></script>
<script type="text/javascript"
  src="http://pagead2.googlesyndication.com/pagead/show_ads.js">
</script></center></div></div><!--END_ND_SUMMARY--></div></div></div>

<div class=CGeneric><div class=CTopic><h3 class=CTitle><a name="Adjusting_Parameters"></a>Adjusting Parameters</h3><div class=CBody><p class=CParagraph>Several different parameters exists in an ANN, these parameters are given defaults in the fann library, but they can be adjusted at runtime.&nbsp; There is no sense in adjusting most of these parameters after the training, since it would invalidate the training, but it does make sense to adjust some of the parameters during training, as will be described in Training and Testing.&nbsp; Generally speaking, these are parameters that should be adjusted before training.</p><p class=CParagraph>The training algorithm is one of the most important parameters.&nbsp; The default training algorithm is FANN_TRAIN_RPROP, but this may not always be the best choice.&nbsp; See <a href="../files/fann_train-h.html#fann_set_training_algorithm" class=LFunction id=link2 onMouseOver="ShowTip(event, 'tt2', 'link2')" onMouseOut="HideTip('tt2')">fann_set_training_algorithm</a> for more information about the different training algorithms.</p><p class=CParagraph>The training algorithms have several different parameters which can be set.&nbsp; For FANN_TRAIN_INCREMENTAL, FANN_TRAIN_BATCH, FANN_TRAIN_QUICKPROP the most important parameter is the learning rate, but unfortunately this is also a parameter which is hard to find a reasonable default for.&nbsp; I (SN) have several times ended up using 0.7, but it is a good idea to test several different learning rates when training a network.&nbsp; It is also worth noting that the activation function has a profound effect on the optimal learning rate [Thimm and Fiesler, 1997].&nbsp; The learning rate can be set by using the <a href="../files/fann_train-h.html#fann_set_learning_rate" class=LFunction id=link3 onMouseOver="ShowTip(event, 'tt3', 'link3')" onMouseOut="HideTip('tt3')">fann_set_learning_rate</a> function.</p><p class=CParagraph>The initial weights are random values between -0.1 and 0.1, if other weights are preferred, the weights can be altered by the <a href="../files/fann-h.html#fann_randomize_weights" class=LFunction id=link4 onMouseOver="ShowTip(event, 'tt4', 'link4')" onMouseOut="HideTip('tt4')">fann_randomize_weights</a> or <a href="../files/fann-h.html#fann_init_weights" class=LFunction id=link5 onMouseOver="ShowTip(event, 'tt5', 'link5')" onMouseOut="HideTip('tt5')">fann_init_weights</a> function.</p><p class=CParagraph>In [Thimm and Fiesler, High-Order and Multilayer Perceptron Initialization, 1997], Thimm and Fiesler state that, &ldquo;An (sic) fixed weight variance of 0.2, which corresponds to a weight range of [-0.77, 0.77], gave the best mean performance for all the applications tested in this study.&nbsp; This performance is similar or better as compared to those of the other weight initialization methods.&rdquo;</p><p class=CParagraph>The standard activation function is the sigmoid activation function, but it is also possible to use other functions.&nbsp; A list of the currently available activation functions is available in the <a href="../files/fann_data-h.html#fann_activationfunc_enum" class=LType id=link6 onMouseOver="ShowTip(event, 'tt6', 'link6')" onMouseOut="HideTip('tt6')">fann_activationfunc_enum</a> section.&nbsp; The activation function can be set for a single neuron using the <a href="../files/fann_train-h.html#fann_set_activation_function" class=LFunction id=link7 onMouseOver="ShowTip(event, 'tt7', 'link7')" onMouseOut="HideTip('tt7')">fann_set_activation_function</a> function and for a group of neurons by the <a href="../files/fann_train-h.html#fann_set_activation_function_hidden" class=LFunction id=link8 onMouseOver="ShowTip(event, 'tt8', 'link8')" onMouseOut="HideTip('tt8')">fann_set_activation_function_hidden</a> and the <a href="../files/fann_train-h.html#fann_set_activation_function_output" class=LFunction id=link9 onMouseOver="ShowTip(event, 'tt9', 'link9')" onMouseOut="HideTip('tt9')">fann_set_activation_function_output</a> functions.&nbsp; Likewise the steepness parameter used in the activation function can be adjusted with the <a href="../files/fann_train-h.html#fann_set_activation_steepness" class=LFunction id=link10 onMouseOver="ShowTip(event, 'tt10', 'link10')" onMouseOut="HideTip('tt10')">fann_set_activation_steepness</a> function.</p><p class=CParagraph>FANN distinguishes between the hidden layers and the output layer, to allow more flexibility.&nbsp; This is especially a good idea for users wanting discrete output from the network, since they can set the activation function for the output to threshold.&nbsp; Please note, that it is not possible to train a network when using the threshold activation function, due to the fact, that it is not differentiable.</p></div></div></div>

<div class=CGeneric><div class=CTopic><h3 class=CTitle><a name="Network_Design"></a>Network Design</h3><div class=CBody><p class=CParagraph>When creating a network it is necessary to define how many layers, neurons and connections it should have.&nbsp; If the network become too large, the ANN will have difficulties learning and when it does learn it will tend to over-fit resulting in poor generalization.&nbsp; If the network becomes too small, it will not be able to represent the rules needed to learn the problem and it will never gain a sufficiently low error rate.</p><p class=CParagraph>The number of hidden layers is also important.&nbsp; Generally speaking, if the problem is simple it is often enough to have one or two hidden layers, but as the problems get more complex, so does the need for more layers.</p><p class=CParagraph>One way of getting a large network which is not too complex, is to adjust the connection_rate parameter given to <a href="../files/fann-h.html#fann_create_sparse" class=LFunction id=link11 onMouseOver="ShowTip(event, 'tt11', 'link11')" onMouseOut="HideTip('tt11')">fann_create_sparse</a>.&nbsp; If this parameter is 0.5, the constructed network will have the same amount of neurons, but only half as many connections.&nbsp; It is difficult to say which problems this approach is useful for, but if you have a problem which can be solved by a fully connected network, then it would be a good idea to see if it still works after removing half the connections.</p><p class=CParagraph>Version 2.0 of the FANN library introduces a new way of creating ANNs, where the neurons are added one by one to the ANN, until it has build an optimal ANN.&nbsp; This approach uses the Cascade2 algorithm and is described in detail in the <a href="../files/fann_cascade-h.html#Cascade_Training" class=LGroup >Cascade Training</a> section.</p></div></div></div>

<div class=CGeneric><div class=CTopic><h3 class=CTitle><a name="Understanding_the_Error_Value"></a>Understanding the Error Value</h3><div class=CBody><p class=CParagraph>The mean square error value is calculated while the ANN is being trained.&nbsp; Some functions are implemented, to use and manipulate this error value.&nbsp; The <a href="../files/fann_train-h.html#fann_get_MSE" class=LFunction id=link12 onMouseOver="ShowTip(event, 'tt12', 'link12')" onMouseOut="HideTip('tt12')">fann_get_MSE</a> function returns the error value and the <a href="../files/fann_train-h.html#fann_reset_MSE" class=LFunction id=link13 onMouseOver="ShowTip(event, 'tt13', 'link13')" onMouseOut="HideTip('tt13')">fann_reset_MSE</a> resets the error value.&nbsp; The following explains how the mean square error value is calculated, to give an idea of the value&rsquo;s ability to reveal the quality of the training.</p><p class=CParagraph>If d is the desired output of an output neuron and y is the actual output of the neuron, the square error is (d <b>-</b> y) squared.&nbsp; If two output neurons exists, then the mean square error for these two neurons is the average of the two square errors.</p><p class=CParagraph>When training with the <a href="../files/fann_train-h.html#fann_train_on_file" class=LFunction id=link14 onMouseOver="ShowTip(event, 'tt1', 'link14')" onMouseOut="HideTip('tt1')">fann_train_on_file</a> function, an error value is printed.&nbsp; This error value is the mean square error for all the training data.&nbsp; Meaning that it is the average of all the square errors in each of the training pairs.</p></div></div></div>

<div class=CGeneric><div class=CTopic><h3 class=CTitle><a name="Training_and_Testing"></a>Training and Testing</h3><div class=CBody><p class=CParagraph>Normally it will be sufficient to use the <a href="../files/fann_train-h.html#fann_train_on_file" class=LFunction id=link15 onMouseOver="ShowTip(event, 'tt1', 'link15')" onMouseOut="HideTip('tt1')">fann_train_on_file</a> training function, but sometimes you want to have more control and you will have to write a custom training loop.&nbsp; This could be because you would like another stop criteria, or because you would like to adjust some of the parameters during training.&nbsp; Another stop criteria than the value of the combined mean square error could be that each of the training pairs should have a mean square error lower than a given value.</p><h4 class=CHeading>A highly simplified version of the <a href="../files/fann_train-h.html#fann_train_on_file" class=LFunction id=link16 onMouseOver="ShowTip(event, 'tt1', 'link16')" onMouseOut="HideTip('tt1')">fann_train_on_file</a> function</h4><blockquote><pre class=CCode>struct fann_train_data *data = fann_read_train_from_file(filename);<br>for(i = 1 ; i &lt;= max_epochs ; i++) {<br>  error = fann_train_epoch(ann, data);<br>  if ( error &lt; desired_error ) {<br>    break;<br>  }<br>}<br>fann_destroy_train(data);</pre></blockquote><p class=CParagraph>This piece of code introduces the <a href="../files/fann_train-h.html#fann_train_epoch" class=LFunction id=link17 onMouseOver="ShowTip(event, 'tt14', 'link17')" onMouseOut="HideTip('tt14')">fann_train_epoch</a> function, which trains the ANN for one epoch and also returns the mean square error.&nbsp; The <a href="../files/fann_train-h.html#struct_fann_train_data" class=LClass id=link18 onMouseOver="ShowTip(event, 'tt15', 'link18')" onMouseOut="HideTip('tt15')">struct fann_train_data</a> structure is also introduced, this structure is a container for the training data.&nbsp; The structure can be used to train the ANN, but it can also be used to test the ANN with data which it has not been trained with.</p><h4 class=CHeading>Test all of the data in a file and calculates the mean square error</h4><blockquote><pre class=CCode>struct fann_train_data *data = fann_read_train_from_file(filename);<br>fann_reset_MSE(ann);<br>fann_test_data(ann, data);<br>printf(&quot;Mean Square Error: %f\n&quot;, fann_get_MSE(ann));<br>fann_destroy_train(data);</pre></blockquote><p class=CParagraph>This piece of code introduces another useful function: <a href="../files/fann_train-h.html#fann_test_data" class=LFunction id=link19 onMouseOver="ShowTip(event, 'tt16', 'link19')" onMouseOut="HideTip('tt16')">fann_test_data</a> function, which updates the mean square error.</p></div></div></div>

<div class=CGeneric><div class=CTopic><h3 class=CTitle><a name="Avoid_Over-Fitting"></a>Avoid Over-Fitting</h3><div class=CBody><p class=CParagraph>With the knowledge of how to train and test an ANN, a new approach to training can be introduced.&nbsp; If too much training is applied to a set of data, the ANN will eventually over-fit, meaning that it will be fitted precisely to this set of training data and thereby loosing generalization.&nbsp; It is often a good idea to test, how good an ANN performs on data that it has not seen before.&nbsp; Testing with data not seen before, can be done while training, to see how much training is required in order to perform well without over-fitting.&nbsp; The testing can either be done by hand, or an automatic test can be applied, which stops the training when the mean square error of the test data is not improving anymore.</p></div></div></div>

<div class=CGeneric><div class=CTopic><h3 class=CTitle><a name="Adjusting_Parameters_During_Training"></a>Adjusting Parameters During Training</h3><div class=CBody><p class=CParagraph>If a very low mean square error is required it can sometimes be a good idea to gradually decrease the learning rate during training, in order to make the adjusting of weights more subtle.&nbsp; If more precision is required, it might also be a good idea to use double precision floats instead of standard floats.</p><p class=CParagraph>The threshold activation function is faster than the sigmoid function, but since it is not possible to train with this function, you may wish to consider an alternate approach.</p><p class=CParagraph>While training the ANN you could slightly increase the steepness parameter of the sigmoid function.&nbsp; This would make the sigmoid function more steep and make it look more like the threshold function.&nbsp; After this training session you could set the activation function to the threshold function and the ANN would work with this activation function.&nbsp; This approach will not work on all kinds of problems, but has been successfully tested on the XOR function.</p></div></div></div>

</td>

</tr></table><div class=Footer><!--START_ND_FOOTER--><div align="center"><a href="#top">TOP</a></div> Created and managed by Steffen Nissen.<br><a href="http://sourceforge.net"><img src="http://sourceforge.net/sflogo.php?group_id=93562&amp;type=1" width="88" height="31" border="0" alt="SourceForge.net Logo" /></a><br>Generated by <a href="http://www.naturaldocs.org">Natural Docs</a>.<!--END_ND_FOOTER--></div>
<!--START_ND_TOOLTIPS-->
<div class=CToolTip id="tt1"><div class=CFunction><blockquote><table border=0 cellspacing=0 cellpadding=0 class=Prototype><tr><td><table border=0 cellspacing=0 cellpadding=0><tr><td class=PBeforeParameters colspan=5>FANN_EXTERNAL void FANN_API fann_train_on_file(</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap>struct&nbsp;</td><td class=PType nowrap>fann&nbsp;</td><td class=PParameterPrefix nowrap>*</td><td class=PParameter nowrap width=100%>ann,</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap>const&nbsp;</td><td class=PType nowrap>char&nbsp;</td><td class=PParameterPrefix nowrap>*</td><td class=PParameter nowrap width=100%>filename,</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap>unsigned&nbsp;</td><td class=PType nowrap>int&nbsp;</td><td class=PParameterPrefix nowrap></td><td class=PParameter nowrap width=100%>max_epochs,</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap>unsigned&nbsp;</td><td class=PType nowrap>int&nbsp;</td><td class=PParameterPrefix nowrap></td><td class=PParameter nowrap width=100%>epochs_between_reports,</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap></td><td class=PType nowrap>float&nbsp;</td><td class=PParameterPrefix nowrap></td><td class=PParameter nowrap width=100%>desired_error</td></tr><tr><td class=PAfterParameters colspan=5>)</td></tr></table></td></tr></table></blockquote>Does the same as fann_train_on_data, but reads the training data directly from a file.</div></div><div class=CToolTip id="tt2"><div class=CFunction><blockquote><table border=0 cellspacing=0 cellpadding=0 class=Prototype><tr><td><table border=0 cellspacing=0 cellpadding=0><tr><td class=PBeforeParameters colspan=5>FANN_EXTERNAL void FANN_API fann_set_training_algorithm(</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap>struct&nbsp;</td><td class=PType nowrap>fann&nbsp;</td><td class=PParameterPrefix nowrap>*</td><td class=PParameter nowrap width=100%>ann,</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap>enum&nbsp;</td><td class=PType nowrap>fann_train_enum&nbsp;</td><td class=PParameterPrefix nowrap></td><td class=PParameter nowrap width=100%>training_algorithm</td></tr><tr><td class=PAfterParameters colspan=5>)</td></tr></table></td></tr></table></blockquote>Set the training algorithm.</div></div><div class=CToolTip id="tt3"><div class=CFunction><blockquote><table border=0 cellspacing=0 cellpadding=0 class=Prototype><tr><td><table border=0 cellspacing=0 cellpadding=0><tr><td class=PBeforeParameters nowrap>FANN_EXTERNAL void FANN_API fann_set_learning_rate(</td><td class=PTypePrefix nowrap>struct&nbsp;</td><td class=PType nowrap>fann&nbsp;</td><td class=PParameterPrefix nowrap>*</td><td class=PParameter nowrap>ann,</td></tr><tr><td></td><td class=PTypePrefix nowrap></td><td class=PType nowrap>float&nbsp;</td><td class=PParameterPrefix nowrap></td><td class=PParameter nowrap>learning_rate</td><td class=PAfterParameters nowrap>)</td></tr></table></td></tr></table></blockquote>Set the learning rate.</div></div><div class=CToolTip id="tt4"><div class=CFunction><blockquote><table border=0 cellspacing=0 cellpadding=0 class=Prototype><tr><td><table border=0 cellspacing=0 cellpadding=0><tr><td class=PBeforeParameters nowrap>FANN_EXTERNAL void FANN_API fann_randomize_weights(</td><td class=PTypePrefix nowrap>struct&nbsp;</td><td class=PType nowrap>fann&nbsp;</td><td class=PParameterPrefix nowrap>*</td><td class=PParameter nowrap>ann,</td></tr><tr><td></td><td class=PTypePrefix nowrap></td><td class=PType nowrap>fann_type&nbsp;</td><td class=PParameterPrefix nowrap></td><td class=PParameter nowrap>min_weight,</td></tr><tr><td></td><td class=PTypePrefix nowrap></td><td class=PType nowrap>fann_type&nbsp;</td><td class=PParameterPrefix nowrap></td><td class=PParameter nowrap>max_weight</td><td class=PAfterParameters nowrap>)</td></tr></table></td></tr></table></blockquote>Give each connection a random weight between <b>min_weight</b> and <b>max_weight</b></div></div><div class=CToolTip id="tt5"><div class=CFunction><blockquote><table border=0 cellspacing=0 cellpadding=0 class=Prototype><tr><td><table border=0 cellspacing=0 cellpadding=0><tr><td class=PBeforeParameters colspan=5>FANN_EXTERNAL void FANN_API fann_init_weights(</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap>struct&nbsp;</td><td class=PType nowrap>fann&nbsp;</td><td class=PParameterPrefix nowrap>*</td><td class=PParameter nowrap width=100%>ann,</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap>struct&nbsp;</td><td class=PType nowrap>fann_train_data&nbsp;</td><td class=PParameterPrefix nowrap>*</td><td class=PParameter nowrap width=100%>train_data</td></tr><tr><td class=PAfterParameters colspan=5>)</td></tr></table></td></tr></table></blockquote>Initialize the weights using Widrow + Nguyen&rsquo;s algorithm.</div></div><div class=CToolTip id="tt6"><div class=CType>The activation functions used for the neurons during training. </div></div><div class=CToolTip id="tt7"><div class=CFunction><blockquote><table border=0 cellspacing=0 cellpadding=0 class=Prototype><tr><td><table border=0 cellspacing=0 cellpadding=0><tr><td class=PBeforeParameters colspan=5>FANN_EXTERNAL void FANN_API fann_set_activation_function(</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap>struct&nbsp;</td><td class=PType nowrap>fann&nbsp;</td><td class=PParameterPrefix nowrap>*</td><td class=PParameter nowrap width=100%>ann,</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap>enum&nbsp;</td><td class=PType nowrap>fann_activationfunc_enum&nbsp;</td><td class=PParameterPrefix nowrap></td><td class=PParameter nowrap width=100%>activation_function,</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap></td><td class=PType nowrap>int&nbsp;</td><td class=PParameterPrefix nowrap></td><td class=PParameter nowrap width=100%>layer,</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap></td><td class=PType nowrap>int&nbsp;</td><td class=PParameterPrefix nowrap></td><td class=PParameter nowrap width=100%>neuron</td></tr><tr><td class=PAfterParameters colspan=5>)</td></tr></table></td></tr></table></blockquote>Set the activation function for neuron number <b>neuron</b> in layer number <b>layer</b>, counting the input layer as layer 0.</div></div><div class=CToolTip id="tt8"><div class=CFunction><blockquote><table border=0 cellspacing=0 cellpadding=0 class=Prototype><tr><td><table border=0 cellspacing=0 cellpadding=0><tr><td class=PBeforeParameters colspan=5>FANN_EXTERNAL void FANN_API fann_set_activation_function_hidden(</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap>struct&nbsp;</td><td class=PType nowrap>fann&nbsp;</td><td class=PParameterPrefix nowrap>*</td><td class=PParameter nowrap width=100%>ann,</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap>enum&nbsp;</td><td class=PType nowrap>fann_activationfunc_enum&nbsp;</td><td class=PParameterPrefix nowrap></td><td class=PParameter nowrap width=100%>activation_function</td></tr><tr><td class=PAfterParameters colspan=5>)</td></tr></table></td></tr></table></blockquote>Set the activation function for all of the hidden layers.</div></div><div class=CToolTip id="tt9"><div class=CFunction><blockquote><table border=0 cellspacing=0 cellpadding=0 class=Prototype><tr><td><table border=0 cellspacing=0 cellpadding=0><tr><td class=PBeforeParameters colspan=5>FANN_EXTERNAL void FANN_API fann_set_activation_function_output(</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap>struct&nbsp;</td><td class=PType nowrap>fann&nbsp;</td><td class=PParameterPrefix nowrap>*</td><td class=PParameter nowrap width=100%>ann,</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap>enum&nbsp;</td><td class=PType nowrap>fann_activationfunc_enum&nbsp;</td><td class=PParameterPrefix nowrap></td><td class=PParameter nowrap width=100%>activation_function</td></tr><tr><td class=PAfterParameters colspan=5>)</td></tr></table></td></tr></table></blockquote>Set the activation function for the output layer.</div></div><div class=CToolTip id="tt10"><div class=CFunction><blockquote><table border=0 cellspacing=0 cellpadding=0 class=Prototype><tr><td><table border=0 cellspacing=0 cellpadding=0><tr><td class=PBeforeParameters nowrap>FANN_EXTERNAL void FANN_API fann_set_activation_steepness(</td><td class=PTypePrefix nowrap>struct&nbsp;</td><td class=PType nowrap>fann&nbsp;</td><td class=PParameterPrefix nowrap>*</td><td class=PParameter nowrap>ann,</td></tr><tr><td></td><td class=PTypePrefix nowrap></td><td class=PType nowrap>fann_type&nbsp;</td><td class=PParameterPrefix nowrap></td><td class=PParameter nowrap>steepness,</td></tr><tr><td></td><td class=PTypePrefix nowrap></td><td class=PType nowrap>int&nbsp;</td><td class=PParameterPrefix nowrap></td><td class=PParameter nowrap>layer,</td></tr><tr><td></td><td class=PTypePrefix nowrap></td><td class=PType nowrap>int&nbsp;</td><td class=PParameterPrefix nowrap></td><td class=PParameter nowrap>neuron</td><td class=PAfterParameters nowrap>)</td></tr></table></td></tr></table></blockquote>Set the activation steepness for neuron number <b>neuron</b> in layer number <b>layer</b>, counting the input layer as layer 0.</div></div><div class=CToolTip id="tt11"><div class=CFunction><blockquote><table border=0 cellspacing=0 cellpadding=0 class=Prototype><tr><td><table border=0 cellspacing=0 cellpadding=0><tr><td class=PBeforeParameters colspan=4>FANN_EXTERNAL struct fann *FANN_API fann_create_sparse(</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap></td><td class=PType nowrap>float&nbsp;</td><td class=PParameter nowrap width=100%>connection_rate,</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap>unsigned&nbsp;</td><td class=PType nowrap>int&nbsp;</td><td class=PParameter nowrap width=100%>num_layers,</td></tr><tr><td>&nbsp;&nbsp;&nbsp;</td><td class=PTypePrefix nowrap></td><td class=PType nowrap>&nbsp;</td><td class=PParameter nowrap width=100%>...</td></tr><tr><td class=PAfterParameters colspan=4>)</td></tr></table></td></tr></table></blockquote>Creates a standard backpropagation neural network, which is not fully connected.</div></div><div class=CToolTip id="tt12"><div class=CFunction><blockquote><table border=0 cellspacing=0 cellpadding=0 class=Prototype><tr><td><table border=0 cellspacing=0 cellpadding=0><tr><td class=PBeforeParameters nowrap>FANN_EXTERNAL float FANN_API fann_get_MSE(</td><td class=PTypePrefix nowrap>struct&nbsp;</td><td class=PType nowrap>fann&nbsp;</td><td class=PParameterPrefix nowrap>*</td><td class=PParameter nowrap>ann</td><td class=PAfterParameters nowrap>)</td></tr></table></td></tr></table></blockquote>Reads the mean square error from the network.</div></div><div class=CToolTip id="tt13"><div class=CFunction><blockquote><table border=0 cellspacing=0 cellpadding=0 class=Prototype><tr><td><table border=0 cellspacing=0 cellpadding=0><tr><td class=PBeforeParameters nowrap>FANN_EXTERNAL void FANN_API fann_reset_MSE(</td><td class=PTypePrefix nowrap>struct&nbsp;</td><td class=PType nowrap>fann&nbsp;</td><td class=PParameterPrefix nowrap>*</td><td class=PParameter nowrap>ann</td><td class=PAfterParameters nowrap>)</td></tr></table></td></tr></table></blockquote>Resets the mean square error from the network.</div></div><div class=CToolTip id="tt14"><div class=CFunction><blockquote><table border=0 cellspacing=0 cellpadding=0 class=Prototype><tr><td><table border=0 cellspacing=0 cellpadding=0><tr><td class=PBeforeParameters nowrap>FANN_EXTERNAL float FANN_API fann_train_epoch(</td><td class=PTypePrefix nowrap>struct&nbsp;</td><td class=PType nowrap>fann&nbsp;</td><td class=PParameterPrefix nowrap>*</td><td class=PParameter nowrap>ann,</td></tr><tr><td></td><td class=PTypePrefix nowrap>struct&nbsp;</td><td class=PType nowrap>fann_train_data&nbsp;</td><td class=PParameterPrefix nowrap>*</td><td class=PParameter nowrap>data</td><td class=PAfterParameters nowrap>)</td></tr></table></td></tr></table></blockquote>Train one epoch with a set of training data.</div></div><div class=CToolTip id="tt15"><div class=CClass>Structure used to store data, for use with training.</div></div><div class=CToolTip id="tt16"><div class=CFunction><blockquote><table border=0 cellspacing=0 cellpadding=0 class=Prototype><tr><td><table border=0 cellspacing=0 cellpadding=0><tr><td class=PBeforeParameters nowrap>FANN_EXTERNAL float FANN_API fann_test_data(</td><td class=PTypePrefix nowrap>struct&nbsp;</td><td class=PType nowrap>fann&nbsp;</td><td class=PParameterPrefix nowrap>*</td><td class=PParameter nowrap>ann,</td></tr><tr><td></td><td class=PTypePrefix nowrap>struct&nbsp;</td><td class=PType nowrap>fann_train_data&nbsp;</td><td class=PParameterPrefix nowrap>*</td><td class=PParameter nowrap>data</td><td class=PAfterParameters nowrap>)</td></tr></table></td></tr></table></blockquote>Test a set of training data and calculates the MSE for the training data.</div></div><!--END_ND_TOOLTIPS-->

<script language=JavaScript><!--
if (browserType) {if (browserVer) {document.write("</div>"); }document.write("</div>");}// --></script></body></html>